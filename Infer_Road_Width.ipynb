{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7472,"status":"ok","timestamp":1652978607155,"user":{"displayName":"Logan Vegna","userId":"17400819969367529937"},"user_tz":240},"id":"FptlnbVUvY_A","outputId":"b25d843f-66a5-4b5b-a06f-9729238517d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/Road-Network-Inference/ScRoadExtractor/DBNet\n","Requirement already satisfied: PyMaxflow in /usr/local/lib/python3.7/dist-packages (1.2.13)\n","/content/drive/MyDrive/Road-Network-Inference/ScRoadExtractor/DBNet/wrapper/bilateralfilter\n","running build\n","running build_py\n","running build_ext\n","running install\n","running build\n","running build_py\n","running build_ext\n","running install_lib\n","running install_egg_info\n","Removing /usr/local/lib/python3.7/dist-packages/bilateralfilter-0.1.egg-info\n","Writing /usr/local/lib/python3.7/dist-packages/bilateralfilter-0.1.egg-info\n","/content/drive/MyDrive/Road-Network-Inference/ScRoadExtractor\n"]}],"source":["# notebook init\n","# Ensure that you have cloned the road-network-inference repo into /content/drive/MyDrive/\n","# Make sure the notebook is running using a GPU\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/Road-Network-Inference/ScRoadExtractor/DBNet\n","!pip install PyMaxflow\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.utils.data as data\n","from torch.autograd import Variable as V\n","from tqdm import tqdm \n","from google.colab.patches import cv2_imshow\n","from skimage.draw import line\n","import cv2\n","import os\n","import math\n","import numpy as np\n","import pandas as pd\n","\n","from time import time\n","from networks.dinknet import ResNet34_EdgeNet\n","\n","\n","\n","%cd /content/drive/MyDrive/Road-Network-Inference/ScRoadExtractor/DBNet/wrapper/bilateralfilter\n","%run /content/drive/MyDrive/Road-Network-Inference/ScRoadExtractor/DBNet/wrapper/bilateralfilter/setup.py build\n","%run /content/drive/MyDrive/Road-Network-Inference/ScRoadExtractor/DBNet/wrapper/bilateralfilter/setup.py install\n","%cd /content/drive/MyDrive/Road-Network-Inference/ScRoadExtractor"]},{"cell_type":"code","source":["# Path to the saved model weights\n","weights = './weights/DBNet_10Cities_zoomed_2.th'\n","# Path to the data csv file which is generated by the dataset creation\n","csv_dir = '/content/drive/MyDrive/Road-Network-Inference/data_backup.csv'\n","# path to the output csv. We highly recommend keeping this the same for road width and lane detection so that the outputs end up in the same file\n","output_csv = '/content/drive/MyDrive/Road-Network-Inference/samples_out.csv'"],"metadata":{"id":"oxdqk0lYNVOp","executionInfo":{"status":"ok","timestamp":1652979001761,"user_tz":240,"elapsed":353,"user":{"displayName":"Logan Vegna","userId":"17400819969367529937"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122,"referenced_widgets":["ebef11e8c31048c9a8085b1f008cd404","ee4ccecbadc647449aaab274d20cc097","2b124ac958454d24ba43cc1608cf113a","43f3c0b0d7e44b249e1b8c9998acf6f6","e89115f2b77e4af6a3d1284327090087","99456a69f78245769d1ba391f061f361","e0305853e49f4fc99a55e9936236102f","a192a12622744586b6b866b2c05a8bec","b387dc9646274775ad01b6889133a27e","0db915dca6b84207bc0e80bfa9e05af1","00f739f8db1c4ae5b6b04d4c6f675b1a"]},"id":"f0wuC8SF4VFZ","outputId":"8f44d7b3-8bfe-4a74-cfbf-759f3422aaa3","executionInfo":{"status":"ok","timestamp":1652979089605,"user_tz":240,"elapsed":32221,"user":{"displayName":"Logan Vegna","userId":"17400819969367529937"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/83.3M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebef11e8c31048c9a8085b1f008cd404"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["4\n","4\n","4\n"]}],"source":["# Measures the road width based on the trained model and outputs it to a csv\n","\n","BATCHSIZE_PER_CARD = 2\n","\n","class TTAFrame():\n","    def __init__(self, net):\n","        self.net = net().cuda()\n","        self.net = torch.nn.DataParallel(self.net, device_ids=range(torch.cuda.device_count()))\n","        \n","    def test_one_img_from_path(self, path, evalmode=True):\n","        if evalmode:\n","            self.net.eval()\n","        batchsize = torch.cuda.device_count() * BATCHSIZE_PER_CARD\n","        if batchsize >= 8:\n","            return self.test_one_img_from_path_1(path)\n","        elif batchsize >= 4:\n","            return self.test_one_img_from_path_2(path)\n","        elif batchsize >= 2:\n","            return self.test_one_img_from_path_4(path)\n","\n","    def test_one_img_from_path_8(self, path):\n","        img = cv2.imread(path)#.transpose(2,0,1)[None]\n","        img90 = np.array(np.rot90(img))\n","        img1 = np.concatenate([img[None],img90[None]])\n","        img2 = np.array(img1)[:,::-1]\n","        img3 = np.array(img1)[:,:,::-1]\n","        img4 = np.array(img2)[:,:,::-1]\n","\n","        img1 = img1.transpose(0,3,1,2)\n","        img2 = img2.transpose(0,3,1,2)\n","        img3 = img3.transpose(0,3,1,2)\n","        img4 = img4.transpose(0,3,1,2)\n","\n","        img1 = V(torch.Tensor(np.array(img1, np.float32)/255.0 * 3.2 -1.6).cuda())\n","        img2 = V(torch.Tensor(np.array(img2, np.float32)/255.0 * 3.2 -1.6).cuda())\n","        img3 = V(torch.Tensor(np.array(img3, np.float32)/255.0 * 3.2 -1.6).cuda())\n","        img4 = V(torch.Tensor(np.array(img4, np.float32)/255.0 * 3.2 -1.6).cuda())\n","\n","        maska = self.net.forward(img1).squeeze().cpu().data.numpy()\n","        maskb = self.net.forward(img2).squeeze().cpu().data.numpy()\n","        maskc = self.net.forward(img3).squeeze().cpu().data.numpy()\n","        maskd = self.net.forward(img4).squeeze().cpu().data.numpy()\n","\n","        mask1 = maska + maskb[:,::-1] + maskc[:,:,::-1] + maskd[:,::-1,::-1]\n","        mask2 = mask1[0] + np.rot90(mask1[1])[::-1,::-1]\n","\n","        return mask2\n","\n","    def test_one_img_from_path_4(self, path):\n","        img = cv2.imread(path)#.transpose(2,0,1)[None]\n","        img90 = np.array(np.rot90(img))\n","        img1 = np.concatenate([img[None], img90[None]])\n","        img2 = np.array(img1)[:, ::-1]\n","        img3 = np.array(img1)[:, :, ::-1]\n","        img4 = np.array(img2)[:, :, ::-1]\n","\n","        img1 = img1.transpose(0, 3, 1, 2)\n","        img2 = img2.transpose(0, 3, 1, 2)\n","        img3 = img3.transpose(0, 3, 1, 2)\n","        img4 = img4.transpose(0, 3, 1, 2)\n","\n","        img1 = V(torch.Tensor(np.array(img1, np.float32)/255.0 * 3.2 -1.6).cuda())\n","        img2 = V(torch.Tensor(np.array(img2, np.float32)/255.0 * 3.2 -1.6).cuda())\n","        img3 = V(torch.Tensor(np.array(img3, np.float32)/255.0 * 3.2 -1.6).cuda())\n","        img4 = V(torch.Tensor(np.array(img4, np.float32)/255.0 * 3.2 -1.6).cuda())\n","\n","        maska, edgea = self.net.forward(img1)\n","        maskb, edgeb = self.net.forward(img2)\n","        maskc, edgec = self.net.forward(img3)\n","        maskd, edged = self.net.forward(img4)\n","\n","        maska = maska.squeeze().cpu().data.numpy()\n","        maskb = maskb.squeeze().cpu().data.numpy()\n","        maskc = maskc.squeeze().cpu().data.numpy()\n","        maskd = maskd.squeeze().cpu().data.numpy()\n","\n","        edgea = edgea.squeeze().cpu().data.numpy()\n","        edgeb = edgeb.squeeze().cpu().data.numpy()\n","        edgec = edgec.squeeze().cpu().data.numpy()\n","        edged = edged.squeeze().cpu().data.numpy()\n","\n","        mask1 = maska + maskb[:, ::-1] + maskc[:, :, ::-1] + maskd[:, ::-1, ::-1]\n","        mask2 = mask1[0] + np.rot90(mask1[1])[::-1, ::-1]\n","\n","        edge1 = edgea + edgeb[:, ::-1] + edgec[:, :, ::-1] + edged[:, ::-1, ::-1]\n","        edge2 = edge1[0] + np.rot90(edge1[1])[::-1, ::-1]\n","\n","        return mask2, edge2\n","    \n","    def test_one_img_from_path_2(self, path):\n","        img = cv2.imread(path)#.transpose(2,0,1)[None]\n","        img90 = np.array(np.rot90(img))\n","        img1 = np.concatenate([img[None], img90[None]])\n","        img2 = np.array(img1)[:, ::-1]\n","        img3 = np.concatenate([img1, img2])\n","        img4 = np.array(img3)[:, :, ::-1]\n","        img5 = img3.transpose(0, 3, 1, 2)\n","        img5 = np.array(img5, np.float32)/255.0 * 3.2 - 1.6\n","        img5 = V(torch.Tensor(img5).cuda())\n","        img6 = img4.transpose(0, 3, 1, 2)\n","        img6 = np.array(img6, np.float32)/255.0 * 3.2 - 1.6\n","        img6 = V(torch.Tensor(img6).cuda())\n","        \n","        maska = self.net.forward(img5).squeeze().cpu().data.numpy()#.squeeze(1)\n","        maskb = self.net.forward(img6).squeeze().cpu().data.numpy()\n","        \n","        mask1 = maska + maskb[:,:,::-1]\n","        mask2 = mask1[:2] + mask1[2:,::-1]\n","        mask3 = mask2[0] + np.rot90(mask2[1])[::-1,::-1]\n","        \n","        return mask3\n","    \n","    def test_one_img_from_path_1(self, path):\n","        img = cv2.imread(path)#.transpose(2,0,1)[None]\n","        \n","        img90 = np.array(np.rot90(img))\n","        img1 = np.concatenate([img[None], img90[None]])\n","        img2 = np.array(img1)[:,::-1]\n","        img3 = np.concatenate([img1,img2])\n","        img4 = np.array(img3)[:,:,::-1]\n","        img5 = np.concatenate([img3,img4]).transpose(0,3,1,2)\n","        img5 = np.array(img5, np.float32)/255.0 * 3.2 -1.6\n","        img5 = V(torch.Tensor(img5).cuda())\n","        \n","        mask = self.net.forward(img5).squeeze().cpu().data.numpy()#.squeeze(1)\n","        mask1 = mask[:4] + mask[4:,:,::-1]\n","        mask2 = mask1[:2] + mask1[2:,::-1]\n","        mask3 = mask2[0] + np.rot90(mask2[1])[::-1,::-1]\n","        \n","        return mask3\n","\n","    def load(self, path):\n","        self.net.load_state_dict(torch.load(path))\n","\n","maindir = '/content/drive/MyDrive/Road-Network-Inference/'\n","\n","if os.path.exists(output_csv):\n","  csv_dir = output_csv\n","df = pd.read_csv(csv_dir)\n","\n","solver = TTAFrame(ResNet34_EdgeNet)\n","\n","solver.load(weights)\n","tic = time()\n","\n","output = []\n","output.append(df.columns.values.tolist() + [\"predicted_road_width_meters\"])\n","for i, row in df.iterrows():\n","    \n","  sat_name = row['sat_name']\n","  city_name = row['city_name']\n","  lat = row['n1_lat']\n","  gsd = 156543.03392 * math.cos(lat * math.pi / 180) / math.pow(2, 20) \n","  x1 = int(row['perp1_x'])\n","  y1 = int(row['perp1_y'])\n","  x2 = int(row['perp2_x'])\n","  y2 = int(row['perp2_y'])\n","  \n","  \n","  midpoint = (int((x1+x2)/2), int((y1+y2)/2))\n","  rr, cc = line(midpoint[0],midpoint[1],x2,y2)\n","  points1 = list(zip(rr,cc))\n","  rr, cc = line(midpoint[0],midpoint[1],x1,y1)\n","  points2 = list(zip(rr,cc))\n","  mask, edge = solver.test_one_img_from_path(maindir + sat_name)\n","\n","  mask_binary = mask.copy()\n","  mask_binary[mask_binary > 4] = 255\n","  mask_binary[mask_binary <= 4] = 0\n","  for y, x in points1:\n","    if (mask_binary[x][y] ==0):\n","      x1 = x\n","      y1 = y\n","      break\n","  for y, x in points2:\n","  \n","    if (mask_binary[x][y] ==0):\n","      x2 = x\n","      y2 = y\n","      break\n","  dist = ((x2-x1)**2 + (y2-y1)**2)**.5\n","  print\n","  output.append(row.values.flatten().tolist() + [gsd*dist])\n","\n","np.savetxt(output_csv, output, delimiter=',', fmt='%s')\n","\n","\n","\n","\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Infer_Road_Width.ipynb","provenance":[{"file_id":"13Mwkh680eSQo4K5buWF8MBdd45J0L9Js","timestamp":1652976582450}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"ebef11e8c31048c9a8085b1f008cd404":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ee4ccecbadc647449aaab274d20cc097","IPY_MODEL_2b124ac958454d24ba43cc1608cf113a","IPY_MODEL_43f3c0b0d7e44b249e1b8c9998acf6f6"],"layout":"IPY_MODEL_e89115f2b77e4af6a3d1284327090087"}},"ee4ccecbadc647449aaab274d20cc097":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_99456a69f78245769d1ba391f061f361","placeholder":"​","style":"IPY_MODEL_e0305853e49f4fc99a55e9936236102f","value":"100%"}},"2b124ac958454d24ba43cc1608cf113a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a192a12622744586b6b866b2c05a8bec","max":87319819,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b387dc9646274775ad01b6889133a27e","value":87319819}},"43f3c0b0d7e44b249e1b8c9998acf6f6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0db915dca6b84207bc0e80bfa9e05af1","placeholder":"​","style":"IPY_MODEL_00f739f8db1c4ae5b6b04d4c6f675b1a","value":" 83.3M/83.3M [00:07&lt;00:00, 17.4MB/s]"}},"e89115f2b77e4af6a3d1284327090087":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99456a69f78245769d1ba391f061f361":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0305853e49f4fc99a55e9936236102f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a192a12622744586b6b866b2c05a8bec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b387dc9646274775ad01b6889133a27e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0db915dca6b84207bc0e80bfa9e05af1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00f739f8db1c4ae5b6b04d4c6f675b1a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}